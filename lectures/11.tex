% 2023.05.15 lecture 11 (stepik, online)
\documentclass[../main.tex]{subfiles}
\begin{document}
\begin{exmpl}
 Пусть случайные величины $ \xi_1, \xi_2, \xi_3, \ldots $ независимы, одинаково распределены, имеют матожидание $ \E\xi_n = a $ и дисперсию $ \D\xi_n = \sigma^{2} $. Пусть $ N $ --- дискретная случайная величина, принимающая целые неотрицательные значения. Рассмотрим случайную величину
 \begin{align*}
  S = \xi_1  + \xi_2 + \ldots + \xi_N.
 \end{align*} Чему равно $ \E S $ и $ \D S $ в терминах $ N $?

 \begin{itemize}
  \item Найдём сначала $ \E S $. По свойству \ref{umo_property_4} условного математического ожидания имеем
   \begin{align*}
    \E S &= \E(\E(S \mid N)) = \E \left( \sum_{n=0}^{\infty} \E(S \mid N = n) \Ind_{\left\{ N = n \right\}} \right) = \\
    &= \sum_{n=0}^{\infty} \E(S \mid N = n) \cdot P(N = n).
   \end{align*} Найдём условное математическое ожидание $ S $ относительно событий $ N = n $:
   \begin{align*}
    \E(S \mid N = n) = \E(\xi_1 + \ldots + \xi_n) = na.
   \end{align*} Подставим:
   \begin{align*}
    \E S = \sum_{n=0}^{\infty}na \cdot P(N = n) = a \sum_{n=0}^{\infty} n P(N = n) = a \E N.
   \end{align*} Ответ $ \E S = a \E N$ соответствует интуиции: в среднем будет $ \E N $ случайных величин, и у каждой матожидание равно $ a $. Более того, здесь мы не пользовались независимостью $ \xi_n $.
  \item Теперь найдём дисперсию $ \D S $. Так как мы уже знаем $ \E S $, достаточно найти $ \E S^{2} $. Поступим так же:
   \begin{align*}
    \E S^{2} = \E(\E(S^{2}\mid N)) = \sum_{n=0}^{\infty} \E(S^{2} \mid N = n) \cdot P(N = n).
   \end{align*} Найдём УМО $ S^{2} $ относительно событий $ N = n $:
   \begin{align*}
    \E(S^{2} \mid N = n) &= \E(\xi_1 + \xi_2 + \ldots + \xi_n)^{2} = \sum_{k=1}^{n} \E \xi_k^{2} + \sum_{k \neq j} \E\xi_k \cdot \E\xi_j,
   \end{align*} где в последнем переходе мы воспользовались независимостью $ \xi_k $ и $ \xi_j $. Так как $ \E\xi_k^{2} = \D\xi_k + (\E\xi_k)^{2} $, то
   \begin{align*}
    \E (S^{2} \mid N = n) &= \sum_{k=1}^{n} \D\xi_k + \sum_{k=1}^{n} (\E\xi_k)^{2} + \sum_{k \neq j} \E\xi_k \cdot \E\xi_j = \\
    &= n\sigma^{2} + na^{2} + n(n-1)a^{2} = n\sigma^{2} + n^{2}a^{2}.
   \end{align*} Подставим:
   \begin{align*}
    \E S^{2} &= \sum_{n=0}^{\infty}(n\sigma^{2} + n^{2}a^{2})P(N=n) = \\
    &= \sigma^{2} \sum_{n=0}^{\infty} nP(N=n) + a^{2} \sum_{n=0}^{\infty} n^{2}P(N=n) = \\
    &= \sigma^{2}\E N + a^{2}\E N^{2} = \sigma^{2}\E N + a^{2}\D N + a^{2}(\E N)^{2}.
   \end{align*} Наконец, выразим дисперсию $ S $:
   \begin{align*}
    \D S &= \E S^{2} - (\E S)^{2} = \sigma^{2}\E N + a^{2}\D N + a^{2}(\E N)^{2} - a^{2}(\E N)^{2} = \\
    &= \sigma^{2}\E N + a^{2} \D N.
   \end{align*}
 \end{itemize} Таким образом, $ \E S = a \E N $, $ \D S = \sigma^{2}\E N  + a^{2} \D N $.
\end{exmpl}


\begin{exmpl}
 Пусть случайные величины $ \xi $ и $ \eta $ независимы и одинаково распределены. Чему равно $ \E(\xi \mid \xi + \eta) $?

 Докажем следующее важное равенство:
 \begin{align}
  \label{example:umo_of_two_nor_values}
  \E(\xi\mid\xi+\eta)=\E(\eta\mid\xi+\eta).
 \end{align}

 Обе эти случайные величины по построению измеримы относительно $ \sigma(\xi+\eta) $, поэтому для всякого измеримого множества $ A \in \sigma(\xi + \eta) $ необходимо проверить два равенства
 \begin{align*}
  \E(\E(\xi \mid \xi + \eta) \cdot \Ind_A) &= \E(\eta \cdot \Ind_A), \\
  \E(\E(\eta \mid \xi + \eta) \cdot \Ind_A) &= \E(\xi \cdot \Ind_A).
 \end{align*} Так как по определению мы имеем
 \begin{align*}
  \E(\E(\xi \mid \xi + \eta) \cdot \Ind_A) &= \E(\xi \cdot \Ind_A), \\
  \E(\E(\eta \mid \xi + \eta) \cdot \Ind_A) &= \E(\eta \cdot \Ind_A),
 \end{align*} то достаточно проверить
 \begin{align*}
  \E(\xi \cdot \Ind_A) = \E(\eta \cdot \Ind_A)
 \end{align*} для любого $ A \in \sigma(\xi + \eta) $. Всякое такое $ A $ имеет вид $ A = \left\{ \xi + \eta \in B \right\} $, $ B \in \B_1 $. Поэтому,
 \begin{align*}
  \E(\xi \cdot \Ind_A) &= \E(\xi \cdot \Ind_{\left\{ \xi+\eta \in B \right\}}) = \int_{\R^{2}} x \cdot \Ind_{\left\{ x + y \in B \right\}}(x, y)\,dP_{(\xi,\eta)} = \int_{\R^{2}} x \cdot \Ind_B(x + y)\,dP_{(\xi,\eta)}.
 \end{align*} Так как $ \xi $ и $ \eta $ независимы и одинаково распределены, то $ P_{(\xi,\eta)} = P \times P$, где $ P = P_\xi = P_\eta $. Тогда заменяя переменные получаем
 \begin{align*}
  \E(\xi \cdot \Ind_A) &= \int_{\R^{2}} x \cdot \Ind_B(x + y) \,dP \,dP = \int_{\R^{2}} y \cdot \Ind_B(x + y)  \,dP\,dP = \E(\eta \cdot \Ind_A).
 \end{align*} Равенство \eqref{example:umo_of_two_nor_values} проверено.

 Теперь воспользуемся линейностью условного математического ожидания (свойство \ref{umo_property_5}):
  \begin{align*}
   \E(\xi \mid\xi+\eta) = \frac{1}{2}\E(\xi\mid\xi+\eta) + \frac{1}{2}\E(\eta\mid\xi+\eta) = \frac{1}{2}\E(\xi + \eta \mid \xi + \eta) = \frac{\xi + \eta}{2}.
  \end{align*}
\end{exmpl}

\begin{exmpl}[производящая функция случайного числа случайных величин]
 \label{example:genfun_of_random_number_of_random_values}
 Пусть случайные величины $ \xi_1, \xi_2, \ldots $  независимы, одинаково распределены и принимают целые неотрицательные значения. Пусть $ F $ --- их производящая функция.

 Пусть  $ N $  --- случайная величина, принимающая целые неотрицательные значения, и пусть $ G $  --- производящая функция $ N $ . Обозначим
 \begin{align*}
  S = \xi_1 + \xi_2 + \ldots + \xi_N.
 \end{align*}

 Найдём производящую функцию $ H $ случайной величины $ S $:
 \begin{align*}
  H(z) &= \E z^{S} = \E(\E(z^{S} \mid N)) = \sum_{n=0}^{\infty} P(N=n) \E(z^{S} \mid N = n) = \\
  &= \sum_{n=0}^{\infty}P(N=n) \E z^{\xi_1+\ldots+\xi_n} = \sum_{n=0}^{\infty} P(N=n) \E z^{\xi_1} \cdot \ldots \cdot \E z^{\xi_n} = \\
  &= \sum_{n=0}^{\infty} P(N=n) (F(z))^{n} = G(F(z)).
 \end{align*} Получили красивую формулу $ H(z) = G(F(z)) $.
\end{exmpl}

\newpage
\section{Ветвящийся процесс.}

Рассмотрим первый пример дискретного случайного процесса, который называется \textit{ветвящийся процесс}.

Предположим, у нас есть множество неразличимых частиц. На каждом шаге каждая частица может разделится на несколько частиц: с вероятностью $ f_k $,  $ k \geqslant 0 $ каждая частица превращается в $ k $  частиц.  В частности, частица может исчезнуть с вероятностью $ f_0 $. Разумеется, от чисел $ f_k $ мы требуем
\begin{align*}
 \sum_{k=0}^{\infty}f_k = 1, \qquad f_k \geqslant 0.
\end{align*}

Пускай в начальный момент времени у нас есть одна частица. На первом шаге она делится на сколько-то частиц. Те, в свою очередь, на втором шаге делятся ещё на несколько частиц и так далее.

Обозначим за случайную величину $ \eta_n $ количество частиц на шаге $ n $ ($ \eta_0 = 1 $). Так,
\begin{align}
 \label{eq:eta_branching_process}
 \eta_{n+1} = \xi^{(n+1)}_{1} + \xi^{(n+1)}_2 + \ldots \xi^{(n+1)}_{\eta_n},
\end{align} где случайные величины $ \xi^{(n)}_k $ (количество частиц, в которые превратилась $ k $-я частица на шаге $ n $) независимы, и одинаково распределены дискретным образом с вероятностями $ P(\xi^{(n)}_k = m) = f_m $.

Обозначим за $ G_n(z) $ производящую функцию для $ \eta_n $. Попробуем найти $ G_n(z)$. Так как формула \eqref{eq:eta_branching_process} представляет из себя сумму случайного числа случайных величин (пример \ref{example:genfun_of_random_number_of_random_values}), то
\begin{align*}
 G_{n+1}(z) = G_n(G_1(z)),
\end{align*} где \begin{align*}
 G_1(z) = G(z) = \sum_{k=0}^{\infty}f_kz^{k}
\end{align*} --- производящая функция случайных величин $ \xi^{(n)}_k $. Отсюда по индукции верно
\begin{align*}
 G_n(z) = \underbrace{G(G(\ldots}_{n\text{ раз}}(z)\ldots)).
\end{align*}

Исходя из производящей функции мы сразу можем вычислить матожидание количества частиц на шаге $ n $:
\begin{align*}
 \E \eta_n &= G_n'(1) = \left.(G_{n-1}(G(z)))'\right|_{z=1} = \\
  &= G_{n-1}'(\underbrace{G(1)}_{1}) \cdot G'(1) = G_{n-1}'(1) \cdot G'(1) = \E\eta_{n-1} \cdot \E\eta_1.
\end{align*} Значит,
\begin{align*}
 \E\eta_n = (\E\eta_1)^{n}.
\end{align*}

\begin{df*}
 Будем говорить, что ветвящийся процесс \textit{выродился}, если в какой-то момент не осталось частиц (оказалось $ \eta_n = 0 $ для некоторого $ n $). Соответственно, \textit{вероятностью вырождения} ветвящегося процесса назовём вероятность того, что $ \eta_n = 0 $ для некоторого $ n $.
\end{df*}

\begin{thm}
 \label{theorem:stop_branching_process}
 Вероятность вырождения ветвящегося процесса равна наименьшему корня уравнения $ G(x) = x $ на отрезке $ [0,1] $.
\end{thm}

При этом хотя бы один корень на отрезке всегда есть, так как $ G(1) = 1 $ для производящей функции $ G $.

\begin{proof}[\normalfont\textsc{Доказательство теоремы \ref{theorem:stop_branching_process}}]
 Обозначим событие $ A_n = \left\{ \eta_n = 0 \right\} $. Тогда
 \begin{align*}
  P(A_n) = G_n(0)
 \end{align*} Отметим также, что $ A_n \subset A_{n+1} $ (ведь $ \eta_n = 0 $ влечёт $ \eta_{n+1} = 0 $), поэтому $ P(A_n) \leqslant P(A_{n+1}) $. Так как вероятности ограничены единицей, то существует предел
 \begin{align*}
  q = \lim_{n \to \infty} P(A_n) = \lim_{n \to \infty} G_n(0) \in [0,1].
 \end{align*} Так как события $ A_n $ вложены, то
 \begin{align*}
  q = P \left( \bigcup_{n=0}^{\infty} A_n \right),
 \end{align*} то есть $ q $ в точности равно вероятности вырождения ветвящегося процесса.

 Давайте сначала докажем, что $ q $ есть решение уравнения $ G(x) = x $, для этого вычислим $ G(q) $:
 \begin{align*}
  G(q)&=G \left( \lim_{n \to \infty} G_n(0) \right) = \lim_{n \to \infty} G(G_n(0)) = \lim_{n \to \infty} G_{n+1}(0) = q.
 \end{align*} Здесь мы пользовались тем, что производящая функция $ G $ непрерывна.

 Осталось доказать, что $ q $ обязан быть наименьшим корнем. Для этого сначала поймём, что функция $ G $ обязана возрастать: вычислим производную
 \begin{align*}
  G'(x) = \sum_{k=0}^{\infty} k f_k x^{k-1} \geqslant 0
 \end{align*} при $ x \in [0,1] $. Так, $ G $ возрастает на $ [0,1] $.

 Пусть $ y $ --- наименьший корень уравнения $ G(x) = x $ на $ [0,1] $. Отметим, что такой корень обязательно есть по непрерывности $ G  $. Докажем по индукции, что $ G_n(0) \leqslant y $. 
 \begin{itemize}
  \item База $ n=1 $:
   \begin{align*}
    G_1(0) = G(0) \leqslant G(y) = y
   \end{align*}
  \item Переход $ n \mapsto n+1 $. Если $ G_n(0) \leqslant y $, то
   \begin{align*}
    G_{n+1}(0) = G(G_n(0)) \leqslant G(y) = y.
   \end{align*}
 \end{itemize} В таком случае,
 \begin{align*}
  q = \lim_{n \to \infty} G_n(0) \leqslant y,
 \end{align*} что и требовалось доказать.
\end{proof}

\begin{remrk*}
 
Обсудим получившийся результат. В процессе доказательства теоремы \ref{theorem:stop_branching_process} мы показали
\begin{align*}
 G'(x) = \sum_{k=0}^{\infty} k f_k x^{k-1} \geqslant 0 \implies G(x) \text{ возрастает при } x \geqslant 0.
\end{align*} Можно продифференцировать ещё раз, и тем самым показать
\begin{align*}
 G''(x) = \sum_{k=0}^{\infty} k (k-1) f_k x^{k-2} \geqslant 0 \implies G(x) \text{ выпукла при } x \geqslant 0.
\end{align*}

Обозначим за $ m = \E \xi_1^{(1)} $ математическое ожидание количества частиц, получаемых за один шаг из одной частицы. С другой стороны, $ m = G'(1) $ --- угловой коэффициент касательной к графику функции $ G(x) $ в точке $ x = 1 $.

Тогда если $ m \leqslant 1 $, то из-за выпуклости функции $ G(x) $, весь график $ G(x) $ лежит над прямой $ y = x $ на отрезке $ [0,1] $. В этой ситуации заведомо есть лишь один корень уравнения $ G(x) = x $ на отрезке $ [0,1] $, и это $ x = 1 $. Таким образом, при $ m \leqslant 1 $ вероятность вырождения ветвящегося процесса равна $ 1 $.

\begin{figure}[ht]
 \centering
 \incfig{g_convex}
 \caption{Возможные случаи графика $ G(x) $ относительно $ m=G'(1) $.}
 \label{fig:g_convex}
\end{figure}

Если же $ m > 1 $, то в силу выпуклости прямая может пересекать график лишь в двух точках, одна из них --- это $ x = 1 $, а вторая лежит в полуинтервале $ [0,1) $. 

Вторая точка пересечения равна $ (0,0) $, если $ G(0) = 0 $, а это верно, если и только если $ f_0 = 0 $. Таким образом, вероятность вырождения ветвящегося процесса равна нулю, тогда и только тогда, когда никакая частица не может исчезнуть. Если же $ f_0 > 0 $, то вероятность вырождения положительна.

\end{remrk*}

Рассмотрим теперь ветвящийся процесс, в котором матожидание количества частиц, получающихся из одной частицы, равно единице. Мы знаем, что такой процесс (за исключением вырожденного случая $ G(x) = x $, когда $ f_1 = 1 $) вырождается с вероятностью $ 1 $. Обсудим скорость вырождения такого ветвящегося процесса.

\begin{thm}
 Пусть
 \begin{align*}
  m = \E \xi_1^{(1)} = G'(1) = 1, && b = \D \xi_1^{(1)} = G''(1) \in (0, +\infty)
 \end{align*}
 --- математическое ожидание и дисперсия количества частиц, получающихся за один шаг из одной частицы в ветвящемся процессе. Обозначим за $ q_n $ вероятность того, что на шаге $ n $ нет частиц, а за $ \gamma_n $ обозначим вероятность вырождения на шаге $ n $ (то есть на шаге $ n $ ещё были частицы, а на шаге $ n + 1 $ их не стало). Тогда
 \begin{align}
  \label{eq1:thm:speed_of_branching_process} p_n = 1 - q_n \sim \frac{2}{bn}, \\
  \label{eq2:thm:speed_of_branching_process} \gamma_n \sim \frac{2}{bn^{2}}.
 \end{align}
\end{thm}
\begin{proof}[\normalfont\textsc{Доказательство}]
 Заведём вспомогательную функцию $ g(x) = 1 - G(1 - x) $, $ x \in [0,1] $. Заметим, что
 \begin{align*}
  q_n = G_n(0) = G(q_{n-1}), \qquad q_0 = G(0).
 \end{align*} Тогда
 \begin{align*}
  p_n = g(p_{n-1}), \qquad p_0 = g(1) = 1 - G(0).
 \end{align*} Отметим, что поскольку $ q_n $ стремится к $ 1 $ (как к вероятности вырождения процесса), то $ p_n \to 0 $.

 В свою очередь,
 \begin{align*}
  \gamma_n = q_{n+1} - q_n = (1 - q_n) - (1 - q_{n+1}) = p_n - p_{n+1}.
 \end{align*}

 Запишем теперь формулу Тейлора для функции $ g(x) $ при $ x \to 0 $. Во-первых,
 \begin{align*}
  g(0) = 1 - G(1 - 0) = 1 - 1 = 0.
 \end{align*} Во-вторых,\begin{align*}
  g'(x) = G'(1-x),
 \end{align*} и \begin{align*}
  g'(0) = G'(1) = m = 1.
 \end{align*} В-третьих,
 \begin{align*}
  g''(x) = -G''(1-x),
 \end{align*} и \begin{align*}
  g''(0) = -G''(1) = -b.
 \end{align*} Тогда по формуле Тейлора для функции $ g(x) $  при $ x \to 0 $ имеем
 \begin{align}
  \label{eq:taylor_g:speed_of_branching_process}
  g(x) = x - \frac{bx^{2}}{2} + o(x^{2}).
 \end{align} Исходя из формулы \eqref{eq:taylor_g:speed_of_branching_process} будем смотреть на асимптотику $ p_n $ и $ \gamma_n $.

 Рассмотрим величины $ a_n = 1 / p_n $. Тогда
 \begin{align*}
  a_{n+1} - a_n &= \frac{1}{p_{n+1}} - \frac{1}{p_n} = \frac{p_n - p_{n+1}}{p_n p_{n+1}} = \frac{p_n - g(p_n)}{p_n \cdot g(p_n)} = \\
  &= \frac{p_n -p_n + \frac{bp_n^{2}}{2} + o(p_n^{2})}{p_n \cdot \left( p_n + o(p_n) \right)} = \frac{b / 2 + o(1)}{1 + o(1)} \to \frac{b}{2}.
 \end{align*} Следовательно (по теореме Штольца),
 \begin{align*}
  a_n \sim \frac{bn}{2} \implies p_n \sim \frac{2}{bn}.
 \end{align*} Формула \eqref{eq1:thm:speed_of_branching_process} доказана. А формула \eqref{eq2:thm:speed_of_branching_process} просто следует из неё:
 \begin{align*}
  \gamma_n = p_n - p_{n+1} \sim \frac{2}{bn} - \frac{2}{b(n+1)} = \frac{2((n+1) - n)}{bn(n+1)} \sim \frac{2}{bn^{2}}.
 \end{align*}
\end{proof}

\end{document}
